{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDL_B_AS01 - Signal Processing, Pattern Recognition and Machine Learning\n",
    "\n",
    "## Human Activity Recognition Using Smartphones\n",
    "\n",
    "**Dataset:** https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n",
    "\n",
    "**Goal:** Recognize human activity (6 classes) from sensor readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. DATA INSPECTION AND ACQUISITION\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data_set/UCI HAR Dataset/\"\n",
    "\n",
    "SIGNAL_FILES = [\n",
    "    \"body_acc_x\",\n",
    "    \"total_acc_x\",\n",
    "    \"body_gyro_x\" # cant find exact match \"body_gyro_acc_x\"\n",
    "]\n",
    "\n",
    "ACTIVITY_NAMES = {\n",
    "    1: 'WALKING', 2: 'WALKING_UPSTAIRS', 3: 'WALKING_DOWNSTAIRS',\n",
    "    4: 'SITTING', 5: 'STANDING', 6: 'LYING'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signals(data_path, dataset_type, signal_files):\n",
    "    \"\"\"\n",
    "    Loads specified time-series signals for a given dataset type (train or test).\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The base path to the data directory.\n",
    "        dataset_type (str): 'train' or 'test'.\n",
    "        signal_files (list): List of base names for the signal files (e.g., 'body_acc_x').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are signal names and values are \n",
    "              NumPy arrays of shape (N_instances, 128).\n",
    "    \"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    # Path to the Inertial Signals folder\n",
    "    signals_dir = os.path.join(data_path, dataset_type, 'Inertial Signals')\n",
    "    \n",
    "    print(f\"--- Loading {dataset_type.upper()} Signals from: {signals_dir} ---\")\n",
    "    \n",
    "    for signal_name in signal_files:\n",
    "        # Construct the full filename\n",
    "        filename = f\"{signal_name}_{dataset_type}.txt\"\n",
    "        file_path = os.path.join(signals_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            data_matrix = np.loadtxt(file_path, dtype=np.float32)\n",
    "            signals[signal_name] = data_matrix\n",
    "            print(f\"Loaded {filename}: Shape {data_matrix.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: File not found at {file_path}. Skipping.\")\n",
    "        \n",
    "    return signals\n",
    "\n",
    "\n",
    "def load_labels(data_path, dataset_type):\n",
    "    \"\"\"\n",
    "    Loads the activity labels for a given dataset type\n",
    "    \"\"\"\n",
    "    labels_dir = os.path.join(data_path, dataset_type)\n",
    "    filename = f\"y_{dataset_type}.txt\"\n",
    "    file_path = os.path.join(labels_dir, filename)\n",
    "    \n",
    "    try:\n",
    "        # Labels are single integers per line\n",
    "        labels = np.loadtxt(file_path, dtype=np.int32)\n",
    "        print(f\"Loaded {filename}: Shape {labels.shape}\")\n",
    "        return labels\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Label file not found at {file_path}. Cannot proceed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Data and Labels\n",
    "train_signals = load_signals(DATA_PATH, 'train', SIGNAL_FILES)\n",
    "train_labels = load_labels(DATA_PATH, 'train')\n",
    "\n",
    "# Load Test Data and Labels\n",
    "test_signals = load_signals(DATA_PATH, 'test', SIGNAL_FILES)\n",
    "test_labels = load_labels(DATA_PATH, 'test')\n",
    "\n",
    "# Verify the data\n",
    "if all(s in train_signals for s in SIGNAL_FILES) and train_labels is not None:\n",
    "    expected_train_shape = (7352, 128) # from pdf\n",
    "    first_signal_key = SIGNAL_FILES[0]\n",
    "    \n",
    "    if train_signals[first_signal_key].shape == expected_train_shape:\n",
    "        print(\"\\nTraining data dimensions verified successfully!\")\n",
    "    else:\n",
    "        print(f\"\\nWARNING: Expected training signal shape {expected_train_shape}, but got {train_signals[first_signal_key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization and exploration\n",
    "\n",
    "sensor_key = 'body_acc_x'\n",
    "data_to_plot = train_signals[sensor_key]\n",
    "sample_indices = {}\n",
    "\n",
    "for label_code in range(1, 7):\n",
    "    # Find the index of the first instance that matches the current label\n",
    "    # np.where returns a tuple, we take the first element (the array of indices) \n",
    "    # and then the first index in that array.\n",
    "    index = np.where(train_labels == label_code)[0][0]\n",
    "    sample_indices[label_code] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle(f'Sample Time Series for Sensor: {sensor_key.upper()}', fontsize=16)\n",
    "axes = axes.flatten() # Flatten the 3x2 grid of axes for easy iteration\n",
    "\n",
    "# Time axis for 128 samples\n",
    "time = np.arange(128) \n",
    "\n",
    "for i, (label_code, index) in enumerate(sample_indices.items()):\n",
    "    signal_vector = data_to_plot[index, :]\n",
    "    activity_name = ACTIVITY_NAMES[label_code]\n",
    "    \n",
    "    # Plotting the signal\n",
    "    axes[i].plot(time, signal_vector, linewidth=1.5)\n",
    "    axes[i].set_title(f'Activity {label_code}: {activity_name}')\n",
    "    axes[i].set_xlabel('Sample Number (n)')\n",
    "    axes[i].set_ylabel('Amplitude')\n",
    "    axes[i].grid(True, alpha=0.5)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout to fit suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. YULE-WALKER LINEAR PREDICTION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion on High Order $p$ (Theoretical Question)You are asked: What happens if $p$ is too high? \n",
    "\n",
    "When the predictor order $p$ is increased beyond the actual order of the underlying process:\n",
    "\n",
    "Overfitting: The model starts to fit the noise (the random variations/artifacts) in the signal in addition to the deterministic, underlying pattern. Essentially, the model tries to memorize the training data too well\n",
    "\n",
    "Increased Variance/Instability: The coefficients become less stable (higher variance). Since the Yule-Walker method is non-iterative and assumes the true signal is AR, a very high order means the autocorrelation matrix becomes close to singular (ill-conditioned), leading to unreliable coefficient estimates\n",
    "\n",
    "Noisy Prediction: The predicted signal, while having a very low training error (the error you are minimizing), will perform poorly on new, unseen data. It will incorporate noise that is irrelevant to the true signal structure. For your classification task, high $p$ will lead to feature vectors that are highly sensitive to noise, reducing classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Yule-Walker ---\n",
    "\n",
    "def calculate_autocorrelation(signal, p):\n",
    "    \"\"\"\n",
    "    Calculates the biased autocorrelation estimate r[k] for lags 0 up to p.\n",
    "\n",
    "    Args:\n",
    "        signal (np.array): The 1D input signal (zero-mean assumption, though not strictly required for this estimate).\n",
    "        p (int): The maximum lag (order) to calculate for.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Autocorrelation vector r[0], r[1], ..., r[p]\n",
    "    \"\"\"\n",
    "    N = len(signal)\n",
    "    # Using 'full' mode and taking the positive lags provides a robust way\n",
    "    # to estimate the autocorrelation. We normalize by N (biased estimate).\n",
    "    r_full = np.correlate(signal, signal, mode='full') / N\n",
    "    \n",
    "    # Extract only the positive lags: r[0] is at index N-1, up to r[p]\n",
    "    r = r_full[N-1 : N-1 + p + 1]\n",
    "    return r\n",
    "\n",
    "def solve_yule_walker(r, p):\n",
    "    \"\"\"\n",
    "    Solves the Yule-Walker equations R_p * a = -r_p for the LPC coefficients 'a'.\n",
    "\n",
    "    Args:\n",
    "        r (np.array): Autocorrelation vector r[0] to r[p].\n",
    "        p (int): The order of the predictor.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The LPC coefficients a = [a1, a2, ..., ap].\n",
    "    \"\"\"\n",
    "    # 1. Construct the R_p Toeplitz matrix (p x p)\n",
    "    # R_p = [r[|i-j|]] where i, j are 1 to p.\n",
    "    # The elements are taken from r[0] to r[p-1].\n",
    "    R = np.zeros((p, p))\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            R[i, j] = r[np.abs(i - j)]\n",
    "\n",
    "    # 2. Construct the -r_p vector (p x 1)\n",
    "    # r_p = [r[1], r[2], ..., r[p]]\n",
    "    r_p = -r[1 : p + 1]\n",
    "\n",
    "    # 3. Solve the linear system R * a = r_p\n",
    "    # a_coeffs will be [a1, a2, ..., ap]\n",
    "    try:\n",
    "        a_coeffs = np.linalg.solve(R, r_p)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # If the matrix R is singular or ill-conditioned (rare, but possible)\n",
    "        print(\"Warning: R matrix is singular. Using least-squares approximation.\")\n",
    "        a_coeffs = np.linalg.lstsq(R, r_p, rcond=None)[0]\n",
    "\n",
    "    return a_coeffs\n",
    "\n",
    "def calculate_prediction_error_and_signal(signal, a, p):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error (MSE) of the p-order linear prediction\n",
    "    and returns the full predicted signal vector.\n",
    "    \n",
    "    Args:\n",
    "        signal (np.array): The 1D input signal (length N).\n",
    "        a (np.array): The LPC coefficients [a1, a2, ..., ap] (length p).\n",
    "        p (int): The order of the predictor.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Mean Squared Error (MSE), Predicted Signal Vector)\n",
    "    \"\"\"\n",
    "    N = len(signal)\n",
    "    predicted_signal = np.zeros(N)\n",
    "    \n",
    "    # We use explicit indices for prediction: x_hat[n] = -sum_{k=1}^{p} a_k * x[n-k]\n",
    "    for n in range(p, N):\n",
    "        # The coefficients 'a' are [a1, a2, ..., ap].\n",
    "        \n",
    "        # We need the p previous samples: x[n-1], x[n-2], ..., x[n-p].\n",
    "        # 1. Get the slice of p samples: signal[n-p] up to signal[n-1]\n",
    "        #    This is the slice signal[n-p : n] (forward slice, length p).\n",
    "        # 2. Reverse the slice to align with the coefficients: [::-1]\n",
    "        #    This yields [x[n-1], x[n-2], ..., x[n-p]].\n",
    "        past_samples = signal[n-p : n][::-1] \n",
    "        \n",
    "        # Calculate the predicted sample\n",
    "        prediction = -np.sum(a * past_samples)\n",
    "        predicted_signal[n] = prediction\n",
    "        \n",
    "    # Calculate error (MSE) only on the predicted part (from index p to N)\n",
    "    original_predicted_part = signal[p:]\n",
    "    prediction_actual_part = predicted_signal[p:]\n",
    "    \n",
    "    error = original_predicted_part - prediction_actual_part\n",
    "    mse = np.mean(error**2)\n",
    "    \n",
    "    return mse, predicted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_analyze = train_signals['body_acc_x']\n",
    "\n",
    "N_SAMPLES = data_to_analyze.shape[1] # 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_RANGE = range(2, 21, 2) # Test range p=2, 4, 6, ..., 20\n",
    "N_RANDOM_SAMPLES = 10 # Average over 10 samples\n",
    "all_average_errors = {}\n",
    "random_indices = random.sample(range(data_to_analyze.shape[0]), N_RANDOM_SAMPLES)\n",
    "\n",
    "print(\"--- Step 1: Finding Optimal Predictor Order p ---\")\n",
    "\n",
    "for p in P_RANGE:\n",
    "    current_p_errors = []\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        signal = data_to_analyze[idx, :]\n",
    "        \n",
    "        # Calculate coefficients\n",
    "        r_coeffs = calculate_autocorrelation(signal, p)\n",
    "        a_coeffs = solve_yule_walker(r_coeffs, p)\n",
    "        \n",
    "        # Calculate error\n",
    "        mse, _ = calculate_prediction_error_and_signal(signal, a_coeffs, p)\n",
    "        current_p_errors.append(mse)\n",
    "        \n",
    "    avg_error = np.mean(current_p_errors)\n",
    "    all_average_errors[p] = avg_error\n",
    "    print(f\"Order p={p}: Average MSE = {avg_error:.6f}\")\n",
    "\n",
    "# Determine the best p\n",
    "BEST_P = min(all_average_errors, key=all_average_errors.get)\n",
    "MIN_AVG_ERROR = all_average_errors[BEST_P]\n",
    "\n",
    "print(f\"\\nOptimal Predictor Order: BEST_P = {BEST_P}\")\n",
    "print(f\"Minimum Average MSE: {MIN_AVG_ERROR:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_INDEX = random_indices[0] # Use the first random sample for the plot\n",
    "signal_to_plot = data_to_analyze[PLOT_INDEX, :]\n",
    "\n",
    "# Recalculate coefficients and prediction for the best p\n",
    "r_best = calculate_autocorrelation(signal_to_plot, BEST_P)\n",
    "a_best = solve_yule_walker(r_best, BEST_P)\n",
    "_, predicted_signal_best = calculate_prediction_error_and_signal(signal_to_plot, a_best, BEST_P)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "time_axis = np.arange(N_SAMPLES)\n",
    "plt.plot(time_axis, signal_to_plot, label='Original Data (x[n])', color='C0', linewidth=2)\n",
    "plt.plot(time_axis[BEST_P:], predicted_signal_best[BEST_P:], \n",
    "         label=f'Predicted Data (x̂[n], Order p={BEST_P})', color='C1', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=BEST_P, color='gray', linestyle=':', label='Prediction Start')\n",
    "\n",
    "plt.title(f'Linear Prediction (Yule-Walker) for Best Order p={BEST_P}')\n",
    "plt.xlabel('Sample Number (n)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Analysis Questions\n",
    "\n",
    "##### Q1: What happens when p is too high?\n",
    "\n",
    "Overfitting and Noise Modeling\n",
    "\n",
    "A high-order model gains too many degrees of freedom, allowing it to fit not only the underlying, predictable structure of the signal (the actual dynamics of the human activity) but also the random, non-repeating fluctuations and noise inherent in that specific 128-sample segment. For instance, the model might try to fit a small, unique spike in the signal that is purely noise, rather than a genuine periodic movement\n",
    "\n",
    "Poor Generalization\n",
    "\n",
    "While a high $p$ might lead to a very low prediction error on the 10 training segments used to find the best $p$ (especially if $p$ approaches $N=128$), the resulting LPC coefficients ($\\mathbf{a}$) will be highly sensitive to those specific noise patterns. This means the model loses its ability to generalize to new, unseen segments of the same activity, leading to higher prediction errors when tested on the entire dataset. A model that performs perfectly on training data but poorly on test data is the definition of overfitting\n",
    "\n",
    "Computational and Stability Issues\n",
    "\n",
    "Increasing $p$ significantly increases the size of the Toeplitz autocorrelation matrix Rp\n",
    "\n",
    "##### Q2: How is linear prediction related to AR modeling? When is the output of a linear predictor an AR process?\n",
    "\n",
    "Linear Prediction is the method you use to find the numerical rules (the AR coefficients) that best describe how a signal's past values predict its future values. If we assume a signal is a song for which we try to predict the next note based on the previous: the AR model is the rulebook of the music (\"next note is always a mix of previous five notes with little randomness\") --> true weights and the Linear Prediction is the tool we use to figure out that rulebook by studying a piece of music --> finding the correct weights that minimize the error.\n",
    "\n",
    "The Linear Prediction coefficients define the mathematical Auto Regressive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. KNN ALGORITHM IMPLEMENTATION\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN classifier from scratch\n",
    " \n",
    "def knn_classifier(X_train, y_train, X_test, k=3):\n",
    "    \"\"\"\n",
    "    Implement KNN classification on a set of test data.\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.array): Training feature vectors (N_train, D_features)\n",
    "        y_train (np.array): Training labels (N_train,)\n",
    "        X_test (np.array): Testing feature vectors (N_test, D_features)\n",
    "        k (int): Number of nearest neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Predicted labels for the test set (N_test,)\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    \n",
    "    # Iterate through every instance in the test set\n",
    "    for test_instance in X_test:\n",
    "        distances = []\n",
    "        \n",
    "        # Step 1: Calculate distance to ALL training instances\n",
    "        for i, train_instance in enumerate(X_train):\n",
    "            dist = euclidean_distance(test_instance, train_instance)\n",
    "            # Store the distance and the corresponding training label\n",
    "            distances.append((dist, y_train[i]))\n",
    "            \n",
    "        # Step 2: Find the K-Nearest Neighbors\n",
    "        # Sort the list based on distance (the first element of the tuple)\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Get the labels of the K closest neighbors\n",
    "        neighbors = distances[:k]\n",
    "        neighbor_labels = [label for (dist, label) in neighbors]\n",
    "        \n",
    "        # Step 3: Perform Majority Vote\n",
    "        # Use Counter to find the most common label (class)\n",
    "        # Counter.most_common(1) returns [(most_common_label, count)]\n",
    "        most_common_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "        y_pred.append(most_common_label)\n",
    "        \n",
    "    return np.array(y_pred)\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two vectors (x1 and x2)\n",
    "    using numpy for efficiency.\n",
    "    \n",
    "    The Euclidean distance is defined as: sqrt(sum((x1_k - x2_k)^2))\n",
    "    \"\"\"\n",
    "    # Use numpy's efficient broadcasting and vector operations\n",
    "    squared_diff = (x1 - x2) ** 2\n",
    "    distance = np.sqrt(np.sum(squared_diff))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Total accuracy = (# correctly classified) / (# total instances) * 100%\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the total classification accuracy (in percent).\n",
    "    \"\"\"\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_instances = len(y_true)\n",
    "    accuracy = (correct_predictions / total_instances) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4. KNN BENCHMARK (RAW DATA)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Perform KNN on raw data\n",
    "# - Use all 3 sensor files (body_acc_x, total_acc_x, body_gyro_x)\n",
    "# - Set K=3\n",
    "# - Use Euclidean distance metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Report baseline accuracy\n",
    "# - Calculate and report total classification accuracy\n",
    "# - This serves as benchmark for comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "5. KNN DATA PRE-PROCESSING (NORMALIZATION)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Implement normalization methods\n",
    "# Options to consider:\n",
    "#   - Zero mean and unit standard deviation (z-score)\n",
    "#   - Range normalization to [0, 1] (min-max scaling)\n",
    "#   - Global mean subtraction and global std division\n",
    "\n",
    "def normalize_zscore(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Z-score normalization\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def normalize_minmax(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Min-max normalization to [0, 1]\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Apply normalization to training and test data\n",
    "# - Normalize both train and test sets consistently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Perform KNN with K=3 on normalized data\n",
    "# - Calculate classification accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Comparison with Benchmark\n",
    "\n",
    "Discuss improvement (or lack thereof) compared to raw data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "6. SELECTING MOST IMPORTANT TRAINING SET\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Analyze importance of each sensor type\n",
    "# - body_acc_x: Body acceleration (x-axis)\n",
    "# - total_acc_x: Total acceleration (x-axis)\n",
    "# - body_gyro_x: Body gyroscope (x-axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Selection and Justification\n",
    "\n",
    "Selected two most important files:\n",
    "1. \n",
    "2. \n",
    "\n",
    "Justification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Create reduced training and test sets\n",
    "# - Use only the selected two sensor types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "7. KNN FEATURES DESIGN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Feature Configuration A: LPC Coefficients\n",
    "# - Use Linear Prediction Coding coefficients from step 2\n",
    "# - Extract p coefficients (from optimal order)\n",
    "# - Maximum feature vector length: 32\n",
    "\n",
    "def extract_lpc_features(X, order):\n",
    "    \"\"\"\n",
    "    Extract LPC coefficients as features\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Feature Configuration B: Time Domain Features\n",
    "# Possible features:\n",
    "#   - Mean, Standard deviation, Variance\n",
    "#   - Minimum, Maximum, Range\n",
    "#   - Median, Quartiles\n",
    "#   - Zero-crossing rate\n",
    "#   - Mean crossing rate\n",
    "#   - Skewness, Kurtosis\n",
    "#   - Energy, Entropy\n",
    "#   - Signal magnitude area\n",
    "\n",
    "def extract_time_features(X):\n",
    "    \"\"\"\n",
    "    Extract time domain features\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Feature Configuration C: Frequency Domain Features\n",
    "# - Apply FFT to each signal\n",
    "# - Extract features from frequency spectrum:\n",
    "#   * Spectral energy\n",
    "#   * Spectral entropy\n",
    "#   * Dominant frequency\n",
    "#   * Spectral centroid\n",
    "#   * Power spectral density features\n",
    "#   * Frequency bands energy\n",
    "\n",
    "def extract_frequency_features(X):\n",
    "    \"\"\"\n",
    "    Extract frequency domain features using FFT\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Combined Feature Configurations\n",
    "# - Experiment with combinations of A, B, C\n",
    "# - Design at least 3 different configurations\n",
    "# - Keep total feature vector length ≤ 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 Evaluate each configuration\n",
    "# - Run KNN (K=3) on each feature set\n",
    "# - Report classification accuracy\n",
    "# - Compare and select best configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.6 Theoretical Question\n",
    "\n",
    "Q: Is frequency domain analysis (FFT) related to spectrograms? How are they similar/different?\n",
    "\n",
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "8. KNN FINE-TUNING (HYPERPARAMETER OPTIMIZATION)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Test different values of K\n",
    "# - Try K values: 1, 3, 5, 7, 9, 11, 13, 15, etc.\n",
    "# - Use best feature configuration from step 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Plot K vs Accuracy\n",
    "# - Visualize effect of K on classification accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3 Analysis and Discussion\n",
    "\n",
    "Q1: What happens when K increases? Why?\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "Q2: What happens when K decreases? Why?\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "Q3: How is this related to underfitting/overfitting?\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "Q4: Why select odd values of K?\n",
    "\n",
    "*Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Report optimal K value\n",
    "# - Select K that gives best accuracy\n",
    "# - Report final classification performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "9. RESULTS SUMMARY AND CONCLUSIONS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Summary table of all experiments\n",
    "# - Benchmark accuracy (raw data)\n",
    "# - Normalized data accuracy\n",
    "# - Accuracy with different feature configurations\n",
    "# - Accuracy with different K values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2 Best Configuration\n",
    "\n",
    "Optimal Configuration:\n",
    "- Normalization method: \n",
    "- Feature set: \n",
    "- K value: \n",
    "- Final accuracy: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3 Insights and Conclusions\n",
    "\n",
    "Key Findings:\n",
    "\n",
    "Feature Importance Observations:\n",
    "\n",
    "Lessons Learned:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
